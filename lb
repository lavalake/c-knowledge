#if defined(DHD_LB)
#if !defined(LINUX) && !defined(linux) && !defined(OEM_ANDROID)
#error "DHD Loadbalancing only supported on LINUX | OEM_ANDROID"
#endif /* !LINUX && !OEM_ANDROID */
#include <linux/cpu.h>
#include <bcm_ring.h>
#define DHD_LB_WORKQ_SZ                     (8192)
#define DHD_LB_WORKQ_SYNC           (16)
#define DHD_LB_WORK_SCHED           (DHD_LB_WORKQ_SYNC * 2)
#endif /* DHD_LB */


#if defined(DHD_LB)
/* DHD load balancing: deferral of work to another online CPU */
/* DHD_LB_TXC DHD_LB_RXC DHD_LB_RXP dispatchers, in dhd_linux.c */
extern void dhd_lb_tx_compl_dispatch(dhd_pub_t *dhdp);
extern void dhd_lb_rx_compl_dispatch(dhd_pub_t *dhdp);
extern void dhd_lb_rx_napi_dispatch(dhd_pub_t *dhdp);
extern void dhd_lb_rx_pkt_enqueue(dhd_pub_t *dhdp, void *pkt, int ifidx);

/**
 * dhd_lb_dispatch - load balance by dispatch work to other CPU cores
 * Note: rx_compl_tasklet is dispatched explicitly.
 */
static INLINE void
dhd_lb_dispatch(dhd_pub_t *dhdp, uint16 ring_idx)
{
        switch (ring_idx) {
#if defined(DHD_LB_TXC)
                case BCMPCIE_D2H_MSGRING_TX_COMPLETE:
                        bcm_workq_prod_sync(&dhdp->prot->tx_compl_prod); /* flush WR index */
                        dhd_lb_tx_compl_dispatch(dhdp); /* dispatch tx_compl_tasklet */
                        break;
#endif /* DHD_LB_TXC */

                case BCMPCIE_D2H_MSGRING_RX_COMPLETE:
                {
#if defined(DHD_LB_RXC)
                        dhd_prot_t *prot = dhdp->prot;
                        /* Schedule the takslet only if we have to */
                        if (prot->rxbufpost <= (prot->max_rxbufpost - RXBUFPOST_THRESHOLD)) {
                                /* flush WR index */
                                bcm_workq_prod_sync(&dhdp->prot->rx_compl_prod);
                                dhd_lb_rx_compl_dispatch(dhdp); /* dispatch rx_compl_tasklet */
                        }
#endif /* DHD_LB_RXC */
#if defined(DHD_LB_RXP)
                        dhd_lb_rx_napi_dispatch(dhdp); /* dispatch rx_process_napi */
#endif /* DHD_LB_RXP */
                        break;
                }

                default:
                        break;
        }
}


#if defined(DHD_LB_TXC)
/**
 * DHD load balanced tx completion tasklet handler, that will perform the
 * freeing of packets on the selected CPU. Packet pointers are delivered to
 * this tasklet via the tx complete workq.
 */
void
dhd_lb_tx_compl_handler(unsigned long data)
{
        int elem_ix;
        void *pkt, **elem;
        dmaaddr_t pa;
        uint32 pa_len;
        dhd_pub_t *dhd = (dhd_pub_t *)data;
        dhd_prot_t *prot = dhd->prot;
        bcm_workq_t *workq = &prot->tx_compl_cons;
        uint32 count = 0;

        int curr_cpu;
        curr_cpu = get_cpu();
        put_cpu();

        DHD_LB_STATS_TXC_PERCPU_CNT_INCR(dhd);

        while (1) {
                elem_ix = bcm_ring_cons(WORKQ_RING(workq), DHD_LB_WORKQ_SZ);

                if (elem_ix == BCM_RING_EMPTY) {
                        break;
                }

                elem = WORKQ_ELEMENT(void *, workq, elem_ix);
                pkt = *elem;

                DHD_INFO(("%s: tx_compl_cons pkt<%p>\n", __FUNCTION__, pkt));

                OSL_PREFETCH(PKTTAG(pkt));
                OSL_PREFETCH(pkt);

                pa = DHD_PKTTAG_PA((dhd_pkttag_fr_t *)PKTTAG(pkt));
                pa_len = DHD_PKTTAG_PA_LEN((dhd_pkttag_fr_t *)PKTTAG(pkt));

                DMA_UNMAP(dhd->osh, pa, pa_len, DMA_RX, 0, 0);
#if defined(BCMPCIE) && (defined(LINUX) || defined(OEM_ANDROID))
                dhd_txcomplete(dhd, pkt, true);
#endif /* BCMPCIE && (defined(LINUX) || defined(OEM_ANDROID)) */

                PKTFREE(dhd->osh, pkt, TRUE);
                count++;
        }

        /* smp_wmb(); */
        bcm_workq_cons_sync(workq);
        DHD_LB_STATS_UPDATE_TXC_HISTO(dhd, count);
}
#endif /* DHD_LB_TXC */

#if defined(DHD_LB_RXC)
void
dhd_lb_rx_compl_handler(unsigned long data)
{
        dhd_pub_t *dhd = (dhd_pub_t *)data;
        bcm_workq_t *workq = &dhd->prot->rx_compl_cons;

        DHD_LB_STATS_RXC_PERCPU_CNT_INCR(dhd);

        dhd_msgbuf_rxbuf_post(dhd, TRUE); /* re-use pktids */
        bcm_workq_cons_sync(workq);
}
#endif /* DHD_LB_RXC */
#endif /* DHD_LB */

/**
 * Consume messages out of the D2H ring. Ensure that the message's DMA to host
 * memory has completed, before invoking the message handler via a table lookup
 * of the cmn_msg_hdr::msg_type.
 */
static int BCMFASTPATH
dhd_prot_process_msgtype(dhd_pub_t *dhd, msgbuf_ring_t *ring, uint8 *buf, uint32 len)
{
        uint32 buf_len = len;
        uint16 item_len;
        uint8 msg_type;
        cmn_msg_hdr_t *msg = NULL;
        int ret = BCME_OK;

        ASSERT(ring);
        item_len = ring->item_len;
        if (item_len == 0) {
                DHD_ERROR(("%s: ringidx %d, item_len %d buf_len %d \n",
                        __FUNCTION__, ring->idx, item_len, buf_len));
                return BCME_ERROR;
        }

        while (buf_len > 0) {
                if (dhd->hang_was_sent) {
                        ret = BCME_ERROR;
                        goto done;
                }

                msg = (cmn_msg_hdr_t *)buf;

                /* Wait until DMA completes, then fetch msg_type */
                msg_type = dhd->prot->d2h_sync_cb(dhd, ring, msg, item_len);

                /* Prefetch data to populate the cache */
                OSL_PREFETCH(buf + item_len);

                DHD_TRACE(("msg_type %d item_len %d buf_len %d\n",
                        msg_type, item_len, buf_len));

                if (msg_type == MSG_TYPE_LOOPBACK) {
                        bcm_print_bytes("LPBK RESP: ", (uint8 *)msg, item_len);
                        DHD_ERROR((" MSG_TYPE_LOOPBACK, len %d\n", item_len));
                }

                ASSERT(msg_type < DHD_PROT_FUNCS);
                if (msg_type >= DHD_PROT_FUNCS) {
                        DHD_ERROR(("%s: msg_type %d, item_len %d buf_len %d\n",
                                __FUNCTION__, msg_type, item_len, buf_len));
                        ret = BCME_ERROR;
                        goto done;
                                        }

                if (table_lookup[msg_type]) {
                        table_lookup[msg_type](dhd, buf);
                }

                if (buf_len < item_len) {
                        ret = BCME_ERROR;
                        goto done;
                }
                buf_len = buf_len - item_len;
                buf = buf + item_len;
        }

done:

#ifdef DHD_RX_CHAINING
        dhd_rxchain_commit(dhd);
#endif

#if defined(DHD_LB)
        dhd_lb_dispatch(dhd, ring->idx);
#endif

        return ret;
} /* dhd_prot_process_msgtype */

/** called on MSG_TYPE_TX_STATUS message received from dongle */
static void BCMFASTPATH
dhd_prot_txstatus_process(dhd_pub_t *dhd, void *msg)
{
        dhd_prot_t *prot = dhd->prot;
        host_txbuf_cmpl_t * txstatus;
        unsigned long flags;
        uint32 pktid;
        void *pkt;
        ulong pa;
        uint32 len;
        void *dmah;
        void *secdma;

        /* locks required to protect circular buffer accesses */
        DHD_GENERAL_LOCK(dhd, flags);

        txstatus = (host_txbuf_cmpl_t *)msg;
        pktid = ltoh32(txstatus->cmn_hdr.request_id);

        DHD_TRACE(("txstatus for pktid 0x%04x\n", pktid));
        if (prot->active_tx_count) {
                prot->active_tx_count--;

                /* Release the Lock when no more tx packets are pending */
                if (prot->active_tx_count == 0)
                         DHD_OS_WAKE_UNLOCK(dhd);
        } else {
                DHD_ERROR(("Extra packets are freed\n"));
        }

        ASSERT(pktid != 0);
#if defined(DHD_LB_TXC) && !defined(BCM_SECURE_DMA)
        {
                int elem_ix;
                void **elem;
                bcm_workq_t *workq;
                dmaaddr_t pa;
                uint32 pa_len;

                pkt = DHD_PKTID_TO_NATIVE(dhd, dhd->prot->pktid_map_handle,
                        pktid, pa, pa_len, dmah, secdma, PKTTYPE_DATA_TX);

                workq = &prot->tx_compl_prod;
                /*
                 * Produce the packet into the tx_compl workq for the tx compl tasklet
                 * to consume.
                 */
                OSL_PREFETCH(PKTTAG(pkt));


                /* fetch next available slot in workq */
                elem_ix = bcm_ring_prod(WORKQ_RING(workq), DHD_LB_WORKQ_SZ);

                /* FIXME FABIAN remove this from locker! */
                DHD_PKTTAG_SET_PA((dhd_pkttag_fr_t *)PKTTAG(pkt), pa);
                DHD_PKTTAG_SET_PA_LEN((dhd_pkttag_fr_t *)PKTTAG(pkt), pa_len);

                if (elem_ix == BCM_RING_FULL) {
                        DHD_ERROR(("tx_compl_prod BCM_RING_FULL\n"));
                        goto workq_ring_full;
                }

                elem = WORKQ_ELEMENT(void *, &prot->tx_compl_prod, elem_ix);
                *elem = pkt;

                smp_wmb();

                /* Sync WR index to consumer if the SYNC threshold has been reached */
                if (++prot->tx_compl_prod_sync >= DHD_LB_WORKQ_SYNC) {
                        bcm_workq_prod_sync(workq);
                        prot->tx_compl_prod_sync = 0;
                }

                DHD_INFO(("%s: tx_compl_prod pkt<%p> sync<%d>\n",
                        __FUNCTION__, pkt, prot->tx_compl_prod_sync));

                DHD_GENERAL_UNLOCK(dhd, flags);

                return;
        }
